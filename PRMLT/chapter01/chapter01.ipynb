{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 1.2 Probability Theory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.2 Expectations and covariances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The average value of some function $f(x)$ under a probability distribution $p(x)$ is called the _expectation_ of $f(x)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a discrete distribution\n",
    "$$\n",
    "\\mathop{\\mathbb{E}}[f] = \\sum_{x}p(x)f(x) \\tag{1.33}\n",
    "$$\n",
    "For a continuous distribution\n",
    "$$\n",
    "\\mathop{\\mathbb{E}}[f] = \\int p(x)f(x)dx \\tag{1.34}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we are given a finite number N of points drawn from the probability distribution or probability density, the the _expectation_ can be approximated as a finite sum over these points. The approximation becomes exact in the limit $N\\to\\infty$ \n",
    "$$\\mathop{\\mathbb{E}}[f] \\simeq \\frac{1}{N}\\sum^{N}_{n=1}f(x_{n}) \\tag{1.35}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use a subcript to indicate which variable is being averaged over a functions of several variables. So the _expectation_ of the function $f(x, y)$ with respect to the distribution of $x$ is denoted by $$\\mathop{\\mathbb{E}_{x}}[f(x, y)] \\tag{1.36}$$ \n",
    "Note $\\mathop{\\mathbb{E}_{x}}[f(x, y)]$ will be a function of $y$.\n",
    "And we use $\\mathop{\\mathbb{E}_{x}}[f\\mid y]$ to denote a _conditional expectation_ with repect to a conditional distribution.\n",
    "$$\\mathop{\\mathbb{E}_{x}}[f\\mid y] = \\sum_{x}p(x\\mid y)f(x) = \\int p(x\\mid y)f(x)dx \\tag{1.37}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The covariance and variance is defined by\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "cov(f,g) & = \\mathop{\\mathbb{E}_{x, y}}\\big[(f(x) - \\mathop{\\mathbb{E}}[f(x)])(g(y) - \\mathop{\\mathbb{E}}[g(y)])\\big]\\\\\n",
    "& = \\mathop{\\mathbb{E}_{x, y}}\\big[(f(x)g(y)\\big] - \\mathop{\\mathbb{E}_{x, y}}\\big[f(x)\\mathop{\\mathbb{E}}[g(y)]\\big] - \\mathop{\\mathbb{E}_{x, y}}\\big[g(y)\\mathop{\\mathbb{E}}[f(x)]\\big] + \\mathop{\\mathbb{E}_{x, y}}\\big[\\mathop{\\mathbb{E}}[f(x)]\\mathop{\\mathbb{E}}[g(y)]\\big]\\\\\n",
    "& = \\mathop{\\mathbb{E}_{x, y}}[(f(x)g(y)] - \\mathop{\\mathbb{E}}[f(x)]\\mathop{\\mathbb{E}}[g(y)] - \\mathop{\\mathbb{E}}[g(y)]\\mathop{\\mathbb{E}}[f(x)] + \\mathop{\\mathbb{E}}[f(x)]\\mathop{\\mathbb{E}}[g(y)]\\\\\n",
    "& = \\mathop{\\mathbb{E}_{x, y}}[(f(x)g(y)] - \\mathop{\\mathbb{E}}[f(x)]\\mathop{\\mathbb{E}}[g(y)]\\\\\n",
    "\\end{split}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$var(f) = \\mathop{\\mathbb{E}}\\big[\\big(f(x) - \\mathop{\\mathbb{E}}[f(x)]\\big)^{2}\\big] \\tag{1.38}$$\n",
    "$$var(f) = \\mathop{\\mathbb{E}}[(f(x)^{2}] - \\mathop{\\mathbb{E}}[f(x)]^{2} \\tag{1.39}$$\n",
    "If $f(x) = x$, $g(y) = y$\n",
    "$$var(x) = cov(x, x) = \\mathop{\\mathbb{E}}[x^{2}] - \\mathop{\\mathbb{E}}[x]^{2} \\tag{1.40}$$\n",
    "$$cov(x, y) = \\mathop{\\mathbb{E}_{x, y}}[xy] - \\mathop{\\mathbb{E}}[x]\\mathop{\\mathbb{E}}[y] \\tag{1.41}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for $\\textbf{x}\\in R^{m}$ and $\\textbf{y}\\in R^{n}$, the result is a matrix.\n",
    "$$\n",
    "cov(\\textbf{x}, \\textbf{y}) = \\mathop{\\mathbb{E}_{\\textbf{x}, \\textbf{y}}}[\\textbf{x}\\textbf{y}^{T}] - \\mathop{\\mathbb{E}}[\\textbf{x}]\\mathop{\\mathbb{E}}[\\textbf{y}]^{T} \\tag{1.42}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The covariance matrix generalizes the notion of variance to multiple dimensions.\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "\\Sigma(\\textbf{x}) & = cov(\\textbf{x}, \\textbf{x})\\\\\n",
    "& = \n",
    "\\begin{bmatrix}\n",
    "    \\mathop{\\mathbb{E}}\\bigg[\\Big(x_{1} - \\mathop{\\mathbb{E}}[x_{1}]\\Big)\\Big(x_{1} - \\mathop{\\mathbb{E}}[x_{1}]\\Big)\\bigg] & \\mathop{\\mathbb{E}}\\bigg[\\Big(x_{1} - \\mathop{\\mathbb{E}}[x_{1}]\\Big)\\Big(x_{2} - \\mathop{\\mathbb{E}}[x_{2}]\\Big)\\bigg] & \\dots  & \\mathop{\\mathbb{E}}\\bigg[\\Big(x_{1} - \\mathop{\\mathbb{E}}[x_{1}]\\Big)\\Big(x_{n} - \\mathop{\\mathbb{E}}[x_{n}]\\Big)\\bigg]\\\\\n",
    "    \\mathop{\\mathbb{E}}\\bigg[\\Big(x_{2} - \\mathop{\\mathbb{E}}[x_{2}]\\Big)\\Big(x_{1} - \\mathop{\\mathbb{E}}[x_{1}]\\Big)\\bigg] & \\mathop{\\mathbb{E}}\\bigg[\\Big(x_{2} - \\mathop{\\mathbb{E}}[x_{2}]\\Big)\\Big(x_{2} - \\mathop{\\mathbb{E}}[x_{2}]\\Big)\\bigg] & \\dots  & \\mathop{\\mathbb{E}}\\bigg[\\Big(x_{2} - \\mathop{\\mathbb{E}}[x_{2}]\\Big)\\Big(x_{n} - \\mathop{\\mathbb{E}}[x_{n}]\\Big)\\bigg] \\\\\n",
    "    \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "    \\mathop{\\mathbb{E}}\\bigg[\\Big(x_{n} - \\mathop{\\mathbb{E}}[x_{n}]\\Big)\\Big(x_{1} - \\mathop{\\mathbb{E}}[x_{1}]\\Big)\\bigg] & \\mathop{\\mathbb{E}}\\bigg[\\Big(x_{n} - \\mathop{\\mathbb{E}}[x_{n}]\\Big)\\Big(x_{2} - \\mathop{\\mathbb{E}}[x_{2}]\\Big)\\bigg] & \\dots  & \\mathop{\\mathbb{E}}\\bigg[\\Big(x_{n} - \\mathop{\\mathbb{E}}[x_{n}]\\Big)\\Big(x_{n} - \\mathop{\\mathbb{E}}[x_{n}]\\Big)\\bigg]\n",
    "\\end{bmatrix}\n",
    "\\end{split}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.3 Bayesian probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bayes's theorem allows us to evaluate the uncertainty in $\\textbf{w}$ in the form of the posterior probability $p(\\textbf{w}\\mid \\mathcal{D})$ after we have incorporated the evidence provided by the observed data $\\mathcal{D}$.\n",
    "$$p(\\textbf{w}\\mid \\mathcal{D}) = \\frac{p(\\mathcal{D}\\mid\\textbf{w})p(\\textbf{w})}{p(\\mathcal{D})} \\tag{1.43}$$\n",
    "The quantity $p(\\mathcal{D}\\mid\\textbf{w})$ is called the _likelihood function_. It expresses how probable the observed data ser is for the specified parameter vector $\\textbf{w}$. $p(\\textbf{w})$ is the prior probability distribution of $\\textbf{w}$ before observing the data.\n",
    "We can state Bayes's theorem in words\n",
    "$$posterior \\propto likelihood \\times prior \\tag{1.44}$$\n",
    "The denominator is the normalization constant. which ensures that the posterior distribution integrates to one.\n",
    "$$p(\\mathcal{D}) = \\int p(\\mathcal{D}\\mid\\textbf{w})p(\\textbf{w})d\\textbf{w} \\tag{1.45}$$\n",
    "In a frequentist setting, $\\textbf{w}$ is determined by some form of \"estimator\". A widely used one is _maximum likelihood_, in which $\\textbf{w}$ is set to the value that maximizes the likelihood function $p(\\mathcal{D}\\mid\\textbf{w})$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.4 The Gaussian distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the case of a single real-value variable $x$, the Gaussian distribution is defined by\n",
    "$$\\mathcal{N}(x\\mid\\mu, \\sigma^{2}) = \\frac{1}{(2\\pi\\sigma^{2})^{1/2}}exp\\left\\{-\\frac{1}{2\\sigma^{2}}(x - \\mu)^{2}\\right\\} \\tag{1.46}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\mathop{\\mathbb{E}}[x] = \\mu = \\text{\"mean\"}\\tag{1.49}$$\n",
    "$$\\mathop{\\mathbb{E}}[x^{2}] = \\mu^{2} + \\sigma^{2} \\tag{1.50}$$\n",
    "$$var[x] = \\mathop{\\mathbb{E}}[x^{2}] - \\mathop{\\mathbb{E}}[x]^{2} = \\sigma^{2} =\\text{\"variance\"} \\tag{1.51}$$\n",
    "The reciprocal of the variance, written as $\\beta = \\frac{1}{\\sigma^{2}}$, is called the _precision_."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\mathcal{N}$ defined over a D-dimensional vector x of continuous variables with the covariance $\\Sigma$ is given by\n",
    "$$\\mathcal{N}(x\\mid\\mu, \\Sigma) = \\frac{1}{(2\\pi)^{D/2}}\\frac{1}{\\left|\\Sigma\\right|^{1/2}}exp\\left\\{-\\frac{1}{2}(x - \\mu)^{T}\\Sigma^{-1}(x - \\mu)\\right\\} \\tag{1.52}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Suppose that we have a data set of N observation that are _independent and identically distributed_ $\\textbf{X}$, the using the fact that the _joint probability_ of two independet events is given by the product of marginal probabilities. The probability of the data set, given $\\mu$ and $\\sigma^{2}$ is\n",
    "$$p(\\textbf{X}\\mid\\mu, \\sigma^{2}) = \\prod^{N}_{n=1}\\mathcal{N}(x_{n}|\\mu, \\sigma^{2}) \\tag{1.53}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Taking the log of the likelihhod function, results in the form\n",
    "$$\n",
    "ln\\,p(\\textbf{X}\\mid\\mu, \\sigma^{2}) = -\\frac{1}{2\\sigma^{2}}\\sum^{N}_{n=1}(x_{n} - \\mu)^{2} - \\frac{N}{2} ln\\,\\sigma^{2} - \\frac{N}{2} ln\\,(2\\pi) \\tag{1.54}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maximizing with respect to $\\mu$, we obtain the maximum likelihood solution which is the _sample mean_.\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "\\hat{\\mu} & = \\arg\\max_{\\mu}eq(1.54)\\\\\n",
    "\\implies\\frac{\\partial}{\\partial\\mu}eq(1.54) = 0 & \\implies\\sum^{N}_{n=1}x_{n} = N\\mu\\\\\n",
    "\\end{split}\n",
    "\\end{equation}\n",
    "$$\n",
    "$$\\hat{\\mu} = \\frac{1}{N}\\sum^{N}_{n=1}x_{n} \\tag{1.55}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, maximizing eq(1.54) with respect to $\\sigma^{2}$, we can the _sample variance_.\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "\\hat{\\sigma}^{2} & = \\arg\\max_{\\sigma^{2}}eq(1.54)\\\\\n",
    "\\implies\\frac{\\partial}{\\partial\\sigma^{2}}eq(1.54) = 0 & \\implies\\frac{1}{2(\\sigma^{2})^{2}}\\sum^{N}_{n=1}(x_{n} - \\mu)^{2} = \\frac{N}{2\\sigma^{2}}\\\\\n",
    "\\end{split}\n",
    "\\end{equation}\n",
    "$$\n",
    "$$\\hat{\\sigma}^{2} = \\frac{1}{N}\\sum^{N}_{n=1}(x_{n} - \\hat{\\mu})^{2} \\tag{1.56}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\mathop{\\mathbb{E}}[\\hat{\\mu}] = \\mathop{\\mathbb{E}}\\bigg[\\frac{1}{N}\\sum^{N}_{n=1}x_{n}\\bigg] = \\frac{1}{N}\\sum^{N}_{n=1}\\mathop{\\mathbb{E}}[x_{n}] = \\mu \\tag{1.57}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The maximum likelihood approach systematically underestimates the variance of the distribution by a factor $(N - 1) / N$.\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "\\mathop{\\mathbb{E}}[\\hat{\\sigma}^{2}] & = \\mathop{\\mathbb{E}}\\bigg[\\frac{1}{N}\\sum^{N}_{i=1}(x_{i} - \\frac{1}{N}\\sum^{N}_{j=1}x_{j})^{2}\\bigg]\\\\ \n",
    "& = \\frac{1}{n}\\sum^{N}_{i=1}\\mathop{\\mathbb{E}}\\bigg[x_{i}^{2} - \\frac{2}{N}x_{i}\\sum^{N}_{j=1}x_{j} + \\frac{1}{N^{2}}\\sum^{N}_{j=1}x_{j}\\sum^{N}_{k=1}x_{k}\\bigg]\\\\\n",
    "& = \\frac{1}{n}\\sum^{N}_{i=1}\\bigg[\\frac{N-2}{N}\\mathop{\\mathbb{E}}[x_{i}^{2}] - \\frac{2}{N}\\sum^{N}_{j\\neq i}\\mathop{\\mathbb{E}}[x_{i}x_{j}] + \\frac{1}{N^{2}}\\sum^{N}_{j=1}\\sum^{N}_{k\\neq j}\\mathop{\\mathbb{E}}[x_{j}x_{k}] + \\frac{1}{N^{2}}\\sum^{N}_{j=1}\\mathop{\\mathbb{E}}[x_{j}^{2}]\\bigg]\\\\\n",
    "& = \\frac{1}{n}\\sum^{N}_{i=1}\\bigg[\\frac{N-2}{N}(\\mu^{2} + \\sigma^{2}) - \\frac{2}{N}(N - 1)\\mu^{2} + \\frac{1}{N^{2}}N(N - 1)\\mu^{2} + \\frac{1}{N}(\\mu^{2} + \\sigma^{2})\\bigg]\\\\\n",
    "& = \\frac{N-1}{N}\\sigma^{2}\\\\\n",
    "\\end{split}\n",
    "\\end{equation}\n",
    "$$\n",
    "$$\n",
    "\\mathop{\\mathbb{E}}[\\hat{\\sigma}^{2}] = \\frac{N-1}{N}\\sigma^{2} \\tag{1.58}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is an example of a phenomenon called _bias_ and is related to the problem of over-fitting. The bias of $\\hat{\\theta}$ is defined as\n",
    "$$\n",
    "B(\\hat{\\theta}) = \\mathop{\\mathbb{E}}[\\hat{\\theta} - \\theta] = \\mathop{\\mathbb{E}}[\\hat{\\theta}] - \\theta\n",
    "$$\n",
    "The estimator $\\hat{\\theta}$ is an _unbiased estimator_ of $\\theta$ if and only if $B(\\hat{\\theta}) = 0$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From eq(1.58) it follows that the following estimate for the variance is unbiased\n",
    "$$\n",
    "s^{2} = \\tilde{\\sigma}^{2} = \\frac{N}{N-1}\\hat{\\sigma}^{2} = \\frac{1}{N-1}\\sum^{N}_{n=1}(x_{n} - \\hat{\\mu})^{2} \\tag{1.59}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The bias of the maximum lkelihood solution becomes less significant as the number N of data points icreases, and in the limit $N\\to\\infty$ the maximum likelihood solution for the variance euqals the true variance of the distribution that enerated the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.5 Curve fitting re-visited"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We assume that given the value of _x_, the corresponding value of _t_ has a Gaussian distribution with a mean equal to the value $y(x,\\textbf{w})$. Thus we can express our uncertainty over the value of the target variable using a probability distribution.\n",
    "$$\n",
    "p(t\\mid x, \\textbf{w}, \\beta) = \\mathcal{N}(t\\mid y(x, \\textbf{w}), \\beta^{-1}) \\tag{1.60}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the training data $\\{\\textbf{x},\\textbf{w}\\}$ to determine the values of the unknown parameters $\\textbf{w}$ and $\\beta$ by maximum likelihood. If the data are assumed to be drawn independently from the distribution, then the likelihood function is given by\n",
    "$$\n",
    "p(\\textbf{t}\\mid\\textbf{x},\\textbf{w},\\beta) = \\prod^{N}_{n=1}\\mathcal{N}(t_{n}\\mid y(x_{n}, \\textbf{w}), \\beta^{-1}) \\tag{1.61}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Substituting for the form of the Gaussian distribution, and take the logarithm\n",
    "$$\n",
    "ln\\,p(\\textbf{t}\\mid\\textbf{x},\\textbf{w},\\beta) =  -\\frac{\\beta}{2}\\sum^{N}_{n=1}\\mathcal\\{y(x_{n}, \\textbf{w}) - t_{n}\\}^{2} + \\frac{N}{2}ln\\,\\beta - \\frac{N}{2}ln\\,(2\\pi) \\tag{1.62}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Maximizes likelihood with respect to $\\textbf{w}$ we can obtain the _sum-of-squares-error-function_ defined by eq(1.2). Maximizing likehood with respect to $\\beta$ gives\n",
    "$$\n",
    "\\frac{1}{\\hat{\\beta}} = \\frac{1}{N}\\sum^{N}_{n=1}\\{y(x_{n}, \\hat{\\textbf{w}}) - t_{n}\\}^{2} \\tag{1.63}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having determined the parameters $\\textbf{w}$ and $\\beta$, we can express our probabilistic model in terms of the _predictive distribution_ that gives the probability distribution over _t_.\n",
    "$$p(t\\mid x,\\hat{\\textbf{w}},\\hat{\\beta}) = \\mathcal{N}(t\\mid y(x, \\hat{\\textbf{w}}), \\hat{\\beta}^{-1}) \\tag{1.64}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We introduce a prior distribution over the polynomial coefficients $\\textbf{w}$. Here use a Gaussian distribution just for simplicity.\n",
    "$$\n",
    "p(\\textbf{w}\\mid\\alpha) = \\mathcal{N}(\\textbf{w}\\mid 0, \\alpha^{-1}\\textbf{I}) = \\Big(\\frac{\\alpha}{2\\pi}\\Big)^{(M + 1) / 2} exp\\big\\{-\\frac{\\alpha}{2}\\textbf{w}^{T}\\textbf{w}\\big\\} \\tag{1.65}\n",
    "$$\n",
    "Where $\\alpha$ is the precision of the distribution, and $M + 1$ is the total number of elements in the vector $\\textbf{w}$ for an $M^{th}$ oder polynomial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Variables such as $\\alpha$, which control the distribution of model parameters, are called _hyperparameters_. Using Bayes's theorem\n",
    "$$p(\\textbf{w}\\mid\\textbf{x}, \\textbf{t}, \\alpha, \\beta) \\propto p(\\textbf{t}\\mid\\textbf{x}, \\textbf{w}, \\beta)p(\\textbf{w}\\mid\\alpha)\\tag{1.66}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now determine $\\textbf{w}$ by finding the most probable value of $\\textbf{w}$ by maximizing the posterior distribution.\n",
    "Taking the negative logarithm of eq(1.66), we find that the maximum of the posterior is given by the minimum of\n",
    "$$\\frac{\\beta}{2}\\sum^{N}_{n=1}\\{y(x_{n}, \\hat{\\textbf{w}}) - t_{n}\\}^{2} + \\frac{\\alpha}{2}\\textbf{w}^{T}\\textbf{w}\\tag{1.67}$$\n",
    "eq(1.4) is equivalent to minimize above equation with a regularization parameter given $\\lambda = \\frac{\\alpha}{\\beta}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
