{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_params(parameters):\n",
    "    # Retrieve from parameters\n",
    "    return parameters[\"Wax\"], parameters[\"Waa\"], parameters[\"ba\"], parameters[\"Wya\"], parameters[\"by\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![RNN Cell Forward](rnn/rnn_step_forward.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rnn_cell_forward(Xt, a_prev, parameters):\n",
    "    \"\"\"\n",
    "    Implements a single forward step of the RNN-cell as described in Figure (2)\n",
    "\n",
    "    Arguments:\n",
    "    Xt -- your input data at timestep \"t\", numpy array of shape (n_x, m).\n",
    "    a_prev -- Hidden state at timestep \"t-1\", numpy array of shape (n_a, m)\n",
    "    parameters -- python dictionary containing:\n",
    "                        Wax -- Weight matrix multiplying the input, numpy array of shape (n_a, n_x)\n",
    "                        Waa -- Weight matrix multiplying the hidden state, numpy array of shape (n_a, n_a)\n",
    "                        Wya -- Weight matrix relating the hidden-state to the output, numpy array of shape (n_y, n_a)\n",
    "                        ba --  Bias, numpy array of shape (n_a, 1)\n",
    "                        by -- Bias relating the hidden-state to the output, numpy array of shape (n_y, 1)\n",
    "    Returns:\n",
    "    a_next -- next hidden state, of shape (n_a, m)\n",
    "    yt_hat -- prediction at timestep \"t\", numpy array of shape (n_y, m)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Retrieve parameters from \"parameters\"\n",
    "    Wax, Waa, ba, Wya, by = parse_params(parameters)\n",
    "    \n",
    "    ### START CODE HERE ### (≈2 lines)\n",
    "    # compute next activation state using the formula given above\n",
    "    a_next = np.tanh(Wax.dot(Xt) + Waa.dot(a_prev) + ba) # hidden state\n",
    "\n",
    "    # compute output of the current cell using the formula given above\n",
    "    yt_hat = softmax(Wya.dot(a_next) + by) # unnormalized log probabilities for next element\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return a_next, yt_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_next[4] =  [ 0.59584544  0.18141802  0.61311866  0.99808218  0.85016201  0.99980978\n",
      " -0.18887155  0.99815551  0.6531151   0.82872037]\n",
      "a_next.shape =  (5, 10)\n",
      "yt_pred[1] = [0.9888161  0.01682021 0.21140899 0.36817467 0.98988387 0.88945212\n",
      " 0.36920224 0.9966312  0.9982559  0.17746526]\n",
      "yt_pred.shape =  (2, 10)\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "xt = np.random.randn(3,10)\n",
    "a_prev = np.random.randn(5,10)\n",
    "Waa = np.random.randn(5,5)\n",
    "Wax = np.random.randn(5,3)\n",
    "Wya = np.random.randn(2,5)\n",
    "ba = np.random.randn(5,1)\n",
    "by = np.random.randn(2,1)\n",
    "parameters = {\"Waa\": Waa, \"Wax\": Wax, \"Wya\": Wya, \"ba\": ba, \"by\": by}\n",
    "\n",
    "a_next, yt_pred = rnn_cell_forward(xt, a_prev, parameters)\n",
    "print(\"a_next[4] = \", a_next[4])\n",
    "print(\"a_next.shape = \", a_next.shape)\n",
    "print(\"yt_pred[1] =\", yt_pred[1])\n",
    "print(\"yt_pred.shape = \", yt_pred.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output**: \n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td>\n",
    "            **a_next[4]**:\n",
    "        </td>\n",
    "        <td>\n",
    "           [ 0.59584544  0.18141802  0.61311866  0.99808218  0.85016201  0.99980978\n",
    " -0.18887155  0.99815551  0.6531151   0.82872037]\n",
    "        </td>\n",
    "    </tr>\n",
    "        <tr>\n",
    "        <td>\n",
    "            **a_next.shape**:\n",
    "        </td>\n",
    "        <td>\n",
    "           (5, 10)\n",
    "        </td>\n",
    "    </tr>\n",
    "        <tr>\n",
    "        <td>\n",
    "            **yt[1]**:\n",
    "        </td>\n",
    "        <td>\n",
    "           [ 0.9888161   0.01682021  0.21140899  0.36817467  0.98988387  0.88945212\n",
    "  0.36920224  0.9966312   0.9982559   0.17746526]\n",
    "        </td>\n",
    "    </tr>\n",
    "        <tr>\n",
    "        <td>\n",
    "            **yt.shape**:\n",
    "        </td>\n",
    "        <td>\n",
    "           (2, 10)\n",
    "        </td>\n",
    "    </tr>\n",
    "\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Basic RNN](rnn/rnn.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rnn_predict(X, a0, parameters):\n",
    "    \"\"\"\n",
    "    Implement the forward propagation of the recurrent neural network described in Figure (3).\n",
    "\n",
    "    Arguments:\n",
    "    x -- Input data for every time-step, of shape (n_x, m, T_x).\n",
    "    a0 -- Initial hidden state, of shape (n_a, m)\n",
    "    parameters -- python dictionary containing:\n",
    "                        Waa -- Weight matrix multiplying the hidden state, numpy array of shape (n_a, n_a)\n",
    "                        Wax -- Weight matrix multiplying the input, numpy array of shape (n_a, n_x)\n",
    "                        Wya -- Weight matrix relating the hidden-state to the output, numpy array of shape (n_y, n_a)\n",
    "                        ba --  Bias numpy array of shape (n_a, 1)\n",
    "                        by -- Bias relating the hidden-state to the output, numpy array of shape (n_y, 1)\n",
    "\n",
    "    Returns:\n",
    "    a -- Hidden states for every time-step, numpy array of shape (n_a, m, T_x)\n",
    "    Y_hat -- Predictions for every time-step, numpy array of shape (n_y, m, T_x)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Retrieve dimensions from shapes of x and parameters[\"Wya\"]\n",
    "    n_x, m, T_x = X.shape\n",
    "    n_y, n_a = parameters[\"Wya\"].shape\n",
    "    \n",
    "    # initialize \"a\" and \"y\" with zeros (≈2 lines)\n",
    "    a = np.zeros((n_a, m, T_x))\n",
    "    Y_hat = np.zeros((n_y, m, T_x))\n",
    "    \n",
    "    # Initialize a_next (≈1 line)\n",
    "    a_t = a0\n",
    "    \n",
    "    # loop over all time-steps\n",
    "    for t in range(T_x):\n",
    "        # Update next hidden state, compute the prediction, get the cache (≈1 line)\n",
    "        a_t, yt_hat = rnn_cell_forward(X[:,:,t], a_t, parameters)\n",
    "        # Save the value of the new \"next\" hidden state in a (≈1 line)\n",
    "        a[:,:,t] = a_t\n",
    "        # Save the value of the prediction in y (≈1 line)\n",
    "        Y_hat [:,:,t] = yt_hat\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return a, Y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a[4][1] =  [-0.99999375  0.77911235 -0.99861469 -0.99833267]\n",
      "a.shape =  (5, 10, 4)\n",
      "Y_hat[1][3] = [0.79560373 0.86224861 0.11118257 0.81515947]\n",
      "Y_hat.shape =  (2, 10, 4)\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "x = np.random.randn(3,10,4)\n",
    "a0 = np.random.randn(5,10)\n",
    "Waa = np.random.randn(5,5)\n",
    "Wax = np.random.randn(5,3)\n",
    "Wya = np.random.randn(2,5)\n",
    "ba = np.random.randn(5,1)\n",
    "by = np.random.randn(2,1)\n",
    "parameters = {\"Waa\": Waa, \"Wax\": Wax, \"Wya\": Wya, \"ba\": ba, \"by\": by}\n",
    "\n",
    "a, Y_predict = rnn_predict(x, a0, parameters)\n",
    "print(\"a[4][1] = \", a[4][1])\n",
    "print(\"a.shape = \", a.shape)\n",
    "print(\"Y_hat[1][3] =\", Y_predict[1][3])\n",
    "print(\"Y_hat.shape = \", Y_predict.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output**:\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td>\n",
    "            **a[4][1]**:\n",
    "        </td>\n",
    "        <td>\n",
    "           [-0.99999375  0.77911235 -0.99861469 -0.99833267]\n",
    "        </td>\n",
    "    </tr>\n",
    "        <tr>\n",
    "        <td>\n",
    "            **a.shape**:\n",
    "        </td>\n",
    "        <td>\n",
    "           (5, 10, 4)\n",
    "        </td>\n",
    "    </tr>\n",
    "        <tr>\n",
    "        <td>\n",
    "            **y[1][3]**:\n",
    "        </td>\n",
    "        <td>\n",
    "           [ 0.79560373  0.86224861  0.11118257  0.81515947]\n",
    "        </td>\n",
    "    </tr>\n",
    "        <tr>\n",
    "        <td>\n",
    "            **y.shape**:\n",
    "        </td>\n",
    "        <td>\n",
    "           (2, 10, 4)\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rnn_forward(X, Y, a0, parameters):\n",
    "    \"\"\"\n",
    "    Implement the forward propagation of the recurrent neural network described in Figure (3).\n",
    "\n",
    "    Arguments:\n",
    "    X -- Input data for every time-step, of shape (n_x, m, T_x).\n",
    "    Y -- Output data for every time-step of shape (n_y, m, T_y)\n",
    "    a0 -- Initial hidden state, of shape (n_a, m)\n",
    "    parameters -- python dictionary containing:\n",
    "                        Waa -- Weight matrix multiplying the hidden state, numpy array of shape (n_a, n_a)\n",
    "                        Wax -- Weight matrix multiplying the input, numpy array of shape (n_a, n_x)\n",
    "                        Wya -- Weight matrix relating the hidden-state to the output, numpy array of shape (n_y, n_a)\n",
    "                        ba --  Bias numpy array of shape (n_a, 1)\n",
    "                        by -- Bias relating the hidden-state to the output, numpy array of shape (n_y, 1)\n",
    "\n",
    "    Returns:\n",
    "    loss -- overall sum of loss\n",
    "    a -- hidden state\n",
    "    Y_hat predicted output\n",
    "    \"\"\"\n",
    "    \n",
    "    a, Y_hat = rnn_predict(X, a0, parameters)\n",
    "    \n",
    "    # Cross entropy\n",
    "    loss = np.sum(-Y * np.log(Y_hat) - (1 - Y) * np.log(1 - Y_hat))\n",
    "    \n",
    "    return loss, a, Y_hat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![RNN Backward](rnn/rnn_cell_backprop.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rnn_cell_backward(dy, da_next, Xt, a_prev, at, parameters, gradients):\n",
    "    \"\"\"\n",
    "    Implements the backward pass for the RNN-cell (single time-step).\n",
    "\n",
    "    Arguments:\n",
    "    dy -- Gradient of loos with respect to next layer\n",
    "    da_next -- Gradient of loss with respect to next hidden state\n",
    "    Xt -- input X\n",
    "    a_prev -- hidden state of t-1\n",
    "    at -- hidden state of t\n",
    "\n",
    "    Returns:\n",
    "    gradients -- python dictionary containing:\n",
    "                        dXt -- Gradients of input vector, of shape(n_x, m)\n",
    "                        dWya -- Gradients of hidden-to-output weights, of shape(n_y, n_a)\n",
    "                        dby -- Gradients of bias of output vector, of shape (n_y, 1)\n",
    "                        da_prev -- Gradients of previous hidden state, of shape (n_a, m)\n",
    "                        dWax -- Gradients of input-to-hidden weights, of shape (n_a, n_x)\n",
    "                        dWaa -- Gradients of hidden-to-hidden weights, of shape (n_a, n_a)\n",
    "                        dba -- Gradients of bias vector, of shape (n_a, 1)\n",
    "    \"\"\"\n",
    "    # Retrieve values from parameters\n",
    "    Wax, Waa, ba, Wya, by = parse_params(parameters)\n",
    "    \n",
    "    # Increment global derivatives w.r.t parameters by adding their derivative at time-step t\n",
    "    \n",
    "    gradients['dWya'] += np.dot(dy, at.T)\n",
    "    gradients['dby'] += np.sum(dy, axis=1, keepdims=1)\n",
    "    \n",
    "    da = np.dot(parameters['Wya'].T, dy) + da_next # backprop into tanh\n",
    "    \n",
    "    ### START CODE HERE ###\n",
    "    # compute the gradient of tanh with respect to a_next (≈1 line)\n",
    "    dtanh = (1 - a_t ** 2) * da  # backprop through tanh nonlinearity\n",
    "    \n",
    "    # compute the gradient of the loss with respect to Wax (≈2 lines)\n",
    "    dXt = np.dot(Wax.T, dtanh) \n",
    "    gradients['dWax'] += np.dot(dtanh, Xt.T)\n",
    "    \n",
    "    # compute the gradient with respect to Waa (≈2 lines)\n",
    "    da_prev = np.dot(Waa.T, dtanh)\n",
    "    gradients['dWaa'] += np.dot(dtanh, a_prev.T)\n",
    "    \n",
    "    # compute the gradient with respect to b (≈1 line)\n",
    "    gradients['dba'] += np.sum(dtanh, axis = 1,keepdims=1)\n",
    "\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return gradients, dXt, da_prev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rnn_backward(X, Y, a, Y_hat, parameters):\n",
    "    # Retrieve dimensions from da's and x1's shapes (≈2 lines)\n",
    "    n_a, m, T_x = a.shape\n",
    "    n_x, m, T_x = X.shape\n",
    "    \n",
    "    # Retrieve from parameters\n",
    "    Wax, Waa, ba, Wya, by = parse_params(parameters)\n",
    "    \n",
    "    gradients = {}\n",
    "    \n",
    "    # each one should be initialized to zeros of the same dimension as its corresponding parameter\n",
    "    gradients['dWax'] = np.zeros_like(Wax)\n",
    "    gradients['dWaa'] = np.zeros_like(Waa)\n",
    "    gradients['dWya'] = np.zeros_like(Wya)\n",
    "    gradients['dba'] = np.zeros_like(ba)\n",
    "    gradients['dby'] = np.zeros_like(by)\n",
    "    \n",
    "    dX = np.zeros(X.shape)\n",
    "    da_next = np.zeros((n_a, m))\n",
    "    \n",
    "    ### START CODE HERE ###\n",
    "    # Backpropagate through time\n",
    "    # Loop through all the time steps\n",
    "    for t in reversed(range(T_x)):\n",
    "        # Compute gradients at time step t. Choose wisely the \"da_next\" and the \"cache\" to use in the backward propagation step. (≈1 line)\n",
    "        # Retrieve derivatives from gradients (≈ 1 line)\n",
    "        gradients, dXt, da_next = rnn_cell_backward(Y_hat[:,:,t]-Y[:,:,t], da_next, X[:,:,t], a[:,:,t], a[:,:,t+1], parameters, gradients)\n",
    "        dX[:,:,t] = dXt\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return gradients, dX, da_next"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize(X, Y, a0, parameters, learning_rate = 0.01):\n",
    "    \"\"\"\n",
    "    Execute one step of the optimization to train the model.\n",
    "    \n",
    "    Arguments:\n",
    "    X -- list of integers, where each integer is a number that maps to a character in the vocabulary.\n",
    "    Y -- list of integers, exactly the same as X but shifted one index to the left.\n",
    "    a_0 -- hidden state t=0.\n",
    "    parameters -- python dictionary containing:\n",
    "                        Wax -- Weight matrix multiplying the input, numpy array of shape (n_a, n_x)\n",
    "                        Waa -- Weight matrix multiplying the hidden state, numpy array of shape (n_a, n_a)\n",
    "                        Wya -- Weight matrix relating the hidden-state to the output, numpy array of shape (n_y, n_a)\n",
    "                        ba --  Bias, numpy array of shape (n_a, 1)\n",
    "                        by -- Bias relating the hidden-state to the output, numpy array of shape (n_y, 1)\n",
    "    learning_rate -- learning rate for the model.\n",
    "    \n",
    "    Returns:\n",
    "    loss -- value of the loss function (cross-entropy)\n",
    "    gradients -- python dictionary containing:\n",
    "                        dWax -- Gradients of input-to-hidden weights, of shape (n_a, n_x)\n",
    "                        dWaa -- Gradients of hidden-to-hidden weights, of shape (n_a, n_a)\n",
    "                        dWya -- Gradients of hidden-to-output weights, of shape (n_y, n_a)\n",
    "                        db -- Gradients of bias vector, of shape (n_a, 1)\n",
    "                        dby -- Gradients of output bias vector, of shape (n_y, 1)\n",
    "    a[len(X)-1] -- the last hidden state, of shape (n_a, 1)\n",
    "    \"\"\"\n",
    "    \n",
    "    ### START CODE HERE ###\n",
    "    \n",
    "    # Forward propagate through time (≈1 line)\n",
    "    loss, a, Y_hat = rnn_forward(X, Y, a0, parameters)\n",
    "    \n",
    "    # Backpropagate through time (≈1 line)\n",
    "    np.stack((a0, a), axis=2)\n",
    "    gradients, dX, da0 = rnn_backward(X, Y, a, Y_hat, parameters)\n",
    "    \n",
    "    # Clip your gradients between -5 (min) and 5 (max) (≈1 line)\n",
    "    for gradient in gradients:\n",
    "        np.clip(gradients, -5, 5, out=gradient)\n",
    "    \n",
    "    # Update parameters (≈1 line)\n",
    "    parameters['Wax'] += -lr * gradients['dWax']\n",
    "    parameters['Waa'] += -lr * gradients['dWaa']\n",
    "    parameters['Wya'] += -lr * gradients['dWya']\n",
    "    parameters['ba']  += -lr * gradients['dba']\n",
    "    parameters['by']  += -lr * gradients['dby']\n",
    "    \n",
    "    a0 += -lr * da0\n",
    "    \n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return loss, gradients, a0, dX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(data, n_a, vocab_size, learning_rate, epoch = 1000, batch_size = 1000):\n",
    "    \"\"\"\n",
    "    Trains the model and generates dinosaur names. \n",
    "    \n",
    "    Arguments:\n",
    "    data -- text corpus\n",
    "    idx_to_word -- dictionary that maps the index to a word\n",
    "    word_to_idx -- dictionary that maps a word to an index\n",
    "    num_iterations -- number of iterations to train the model for\n",
    "    n_a -- number of units of the RNN cell\n",
    "    seq_length -- length of sequence you want to sample at each iteration. \n",
    "    vocab_size -- number of unique characters found in the text, size of the vocabulary\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- learned parameters\n",
    "    \"\"\"\n",
    "    \n",
    "    # Retrieve n_x and n_y from vocab_size\n",
    "    n_x = n_y = vocab_size\n",
    "    \n",
    "    # Initialize parameters\n",
    "    parameters = {}\n",
    "    parameters['Wax'] = np.random.randn(n_a, n_x)*0.01 # input to hidden\n",
    "    parameters['Waa'] = np.random.randn(n_a, n_a)*0.01 # hidden to hidden\n",
    "    parameters['Wya'] = np.random.randn(n_y, n_a)*0.01 # hidden to output\n",
    "    parameters['ba'] = np.zeros((n_a, 1)) # hidden bias\n",
    "    parameters['by'] = np.zeros((n_y, 1)) # output bias\n",
    "    \n",
    "    # Initialize loss (this is required because we want to smooth our loss, don't worry about it)\n",
    "    loss = -np.log(1.0 / vocab_size) * n_a\n",
    "    \n",
    "    # Shuffle data\n",
    "    np.random.shuffle(data)\n",
    "    \n",
    "    # Initialize the hidden state of your LSTM\n",
    "    a0 = np.zeros((n_a, 1))\n",
    "    \n",
    "    # Optimization loop\n",
    "    iteration = np.ceil(float(len(data)) / batch)\n",
    "    for i in range(epoch):\n",
    "        for j in range(iteration):\n",
    "            examples = data[j*batch_size:(j+1)*batch_size]\n",
    "            X, Y = np.split(data, 2)\n",
    "            \n",
    "            ### START CODE HERE ###\n",
    "            \n",
    "            # Perform one optimization step: Forward-prop -> Backward-prop -> Clip -> Update parameters\n",
    "            # Choose a learning rate of 0.01\n",
    "            curr_loss, gradients, a_prev = optimize(X, Y, a0, parameters, learning_rate)\n",
    "\n",
    "            ### END CODE HERE ###\n",
    "\n",
    "            # Use a latency trick to keep the loss smooth. It happens here to accelerate the training.\n",
    "            loss = smooth(loss, curr_loss)\n",
    "        \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Sample RNN](rnn/sample_rnn.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(parameters, EOS):\n",
    "    \"\"\"\n",
    "    Sample a sequence of characters according to a sequence of probability distributions output of the RNN\n",
    "\n",
    "    Arguments:\n",
    "    parameters -- python dictionary containing the parameters Waa, Wax, Wya, by, and b. \n",
    "    idx_to_word -- python dictionary mapping each indice to a word\n",
    "\n",
    "    Returns:\n",
    "    indices -- a list of length n containing the indices of the sampled characters.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Retrieve from parameters\n",
    "    Wax, Waa, ba, Wya, by = parse_params(parameters)\n",
    "    vocab_size, n_a = Wya.shape\n",
    "    \n",
    "    ### START CODE HERE ###\n",
    "    # Step 1: Create the one-hot vector x for the first character (initializing the sequence generation). (≈1 line)\n",
    "    x = np.zeros((vocab_size, 1))\n",
    "    # Step 1': Initialize a_prev as zeros (≈1 line)\n",
    "    a_prev = np.zeros((n_a, 1))\n",
    "    \n",
    "    # Create an empty list of indices, this is the list which will contain the list of indices of the characters to generate (≈1 line)\n",
    "    indices = []\n",
    "    \n",
    "    # Idx is a flag to detect a newline character, we initialize it to -1\n",
    "    idx = -1 \n",
    "    \n",
    "    # Loop over time-steps t. At each time-step, sample a character from a probability distribution and append \n",
    "    # its index to \"indices\".\n",
    "    \n",
    "    while (idx != EOF):\n",
    "        \n",
    "        # Step 2: Forward propagate x using the equations (1), (2) and (3)\n",
    "        a = np.tanh(np.dot(Wax, x) + np.dot(Waa, a_prev) + b)\n",
    "        z = np.dot(Wya, a) + by\n",
    "        y = softmax(z)\n",
    "        \n",
    "        # Step 3: Sample the index of a character within the vocabulary from the probability distribution y\n",
    "        idx = np.random.choice(vocab_size, p=y.ravel())\n",
    "\n",
    "        # Append the index to \"indices\"\n",
    "        indices.append(idx)\n",
    "        \n",
    "        # Step 4: Overwrite the input character as the one corresponding to the sampled index.\n",
    "        x = np.zeros((vocab_size, 1))\n",
    "        x[idx] = 1\n",
    "        \n",
    "        # Update \"a_prev\" to be \"a\"\n",
    "        a_prev = a\n",
    "        \n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    if (indices[-1] != EOS):\n",
    "        indices.append(EOS)\n",
    "    \n",
    "    return indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
